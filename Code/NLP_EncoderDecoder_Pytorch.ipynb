{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# META DATA - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
    "\n",
    "    # Developer details: \n",
    "        # Name: Khushboo Mittal, Tanisha Priya, Prachi Tavse, Harshita Jangde\n",
    "        # Role: Architect\n",
    "    # Version:\n",
    "        # Version: V 1.0 (24 October 2024)\n",
    "            # Developers: Khushboo Mittal, Tanisha Priya, Prachi Tavse, Harshita Jangde\n",
    "            # Unit test: Pass\n",
    "            # Integration test: Pass\n",
    "     \n",
    "     # Description: This code snippet implements an Encoder-Decoder model for Natural Language Processing (NLP) \n",
    "     # tasks, focusing on sequence-to-sequence transformation. The model uses embedding layers for input \n",
    "     # encoding, followed by an RNN (GRU) architecture in the encoder to capture sequential dependencies. \n",
    "     # The decoder generates output sequences based on the encoded context. Applications include text translation, \n",
    "     # summarization, and question answering, with support for custom vocabularies and tokenization.\n",
    "\n",
    "# CODE - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
    "\n",
    "    # Dependency: \n",
    "        # Environment:     \n",
    "            # Python: 3.11.5\n",
    "            # torch: 2.0.1\n",
    "            # torchtext: 0.16.0\n",
    "            # scikit-learn: 1.3.2\n",
    "\n",
    "import pandas as pd  # Import pandas for data manipulation\n",
    "from sklearn.model_selection import train_test_split  # Import function for splitting datasets\n",
    "import torch  # Import PyTorch for tensor operations\n",
    "from torchtext.data.utils import get_tokenizer  # Import tokenizer utility from torchtext\n",
    "from torchtext.vocab import build_vocab_from_iterator  # Import function to build vocabulary from text\n",
    "from torch.utils.data import Dataset, DataLoader  # Import classes for dataset handling\n",
    "from torch.nn.utils.rnn import pad_sequence  # Import function to pad sequences for batching\n",
    "from sklearn.metrics import accuracy_score, classification_report # Import the function for evaluation metrics\n",
    "# Define column names for the dataset\n",
    "column_names = ['tweetID', 'entity', 'sentiment', 'tweet_content']\n",
    "\n",
    "# Load the dataset with specified column names; no header in the CSV\n",
    "df = pd.read_csv('../Data/twitter_sentiment_analysis.csv', names=column_names, header=None)\n",
    "df = df[['tweetID', 'sentiment', 'tweet_content']]  # Select only relevant columns\n",
    "\n",
    "# Basic preprocessing of sentiment labels and tweet content\n",
    "df['sentiment'] = df['sentiment'].map({'Positive': 1, 'Negative': 0, 'Neutral': 2, 'Irrelevant': 3})  # Encode sentiments as integers\n",
    "df['tweet_content'] = df['tweet_content'].astype(str)  # Ensure tweet content is of string type\n",
    "df.dropna(subset=['sentiment'], inplace=True)  # Remove rows with missing sentiment values\n",
    "\n",
    "# Split the data into training, validation, and test sets\n",
    "train_df, test_df = train_test_split(df, test_size=0.15, random_state=42)  # 15% of data for testing\n",
    "valid_df, test_df = train_test_split(test_df, test_size=0.5, random_state=42)  # Split remaining data into valid and test sets\n",
    "\n",
    "# Tokenization using a basic English tokenizer\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "# Function to yield tokens for vocabulary creation\n",
    "def yield_tokens(data_iter):\n",
    "    for tweet in data_iter:  # Iterate through each tweet\n",
    "        yield tokenizer(tweet)  # Tokenize and yield tokens\n",
    "\n",
    "# Build vocabulary from the tokenized tweets, including a special unknown token\n",
    "vocab = build_vocab_from_iterator(yield_tokens(df['tweet_content']), specials=[\"<unk>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])  # Set default index for unknown words to the unknown token\n",
    "\n",
    "# Prepare a custom dataset class for loading tweets and their sentiments\n",
    "class TweetDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.dataframe = dataframe  # Store the DataFrame\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)  # Return the size of the dataset\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tweet = self.dataframe.iloc[idx]['tweet_content']  # Get the tweet content\n",
    "        sentiment = self.dataframe.iloc[idx]['sentiment']  # Get the sentiment label\n",
    "        return torch.tensor(vocab(tokenizer(tweet))), torch.tensor(sentiment)  # Return tokenized tweet and sentiment as tensors\n",
    "\n",
    "# Function to collate data into batches for training\n",
    "def collate_fn(batch):\n",
    "    src_batch, trg_batch = zip(*batch)  # Unzip the batch into tweet and sentiment tensors\n",
    "    src_batch = pad_sequence(src_batch, padding_value=vocab[\"<pad>\"], batch_first=True)  # Pad the tweet sequences\n",
    "    trg_batch = torch.stack(trg_batch)  # Stack sentiment labels into a tensor\n",
    "    return src_batch, trg_batch  # Return padded tweets and labels\n",
    "\n",
    "# Create dataset instances for training, validation, and test sets\n",
    "train_dataset = TweetDataset(train_df)\n",
    "valid_dataset = TweetDataset(valid_df)\n",
    "test_dataset = TweetDataset(test_df)\n",
    "\n",
    "# Create DataLoader instances for each dataset to handle batching and shuffling\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn  # Import PyTorch's neural network module\n",
    "\n",
    "# Define the Encoder class\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hidden_dim):\n",
    "        super(Encoder, self).__init__()  # Initialize the parent class\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)  # Embedding layer to convert input tokens to vectors\n",
    "        self.rnn = nn.GRU(emb_dim, hidden_dim, batch_first=True)  # GRU layer with input dimension as embedding size\n",
    "\n",
    "    def forward(self, src):\n",
    "        embedded = self.embedding(src)  # Convert input token indices to embeddings\n",
    "        outputs, hidden = self.rnn(embedded)  # Pass embeddings through the RNN\n",
    "        return hidden  # Return the hidden state for further processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Decoder generates predictions (sentiment labels) based on the hidden state provided by the encoder and the current input token.\n",
    "parameters of intit:\n",
    "\n",
    "output_dim: The size of the output vocabulary (number of unique sentiment labels).\n",
    "\n",
    "emb_dim: The dimension of the embedding vectors (same as in the encoder).\n",
    "\n",
    "hidden_dim: The number of features in the hidden state (same as in the encoder)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Decoder class\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hidden_dim):\n",
    "        super(Decoder, self).__init__()  # Initialize the parent class\n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)  # Embedding layer for output tokens\n",
    "        self.rnn = nn.GRU(emb_dim, hidden_dim, batch_first=True)  # GRU layer for decoding\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)  # Fully connected layer to convert hidden states to output logits\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        input = input.unsqueeze(1)  # Reshape input for RNN: (batch_size, 1)\n",
    "        embedded = self.embedding(input)  # Convert input token indices to embeddings\n",
    "        output, hidden = self.rnn(embedded, hidden)  # Pass embeddings and hidden state through the RNN\n",
    "        prediction = self.fc(output.squeeze(1))  # Convert RNN output to predictions, shape: (batch_size, output_dim)\n",
    "        return prediction, hidden  # Return predictions and the new hidden state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Seq2Seq, self).__init__()  # Initialize the parent class\n",
    "        self.encoder = encoder  # Assign the encoder model\n",
    "        self.decoder = decoder  # Assign the decoder model\n",
    "\n",
    "    def forward(self, src):\n",
    "        hidden = self.encoder(src)  # Encode the source sequence and obtain the final hidden state\n",
    "        input = torch.zeros(src.size(0), dtype=torch.long).to(src.device)  # Create a start token (shape: batch_size)\n",
    "        output, _ = self.decoder(input, hidden)  # Decode using the start token and the hidden state from the encoder\n",
    "        return output  # Return the decoder's output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 1.3211\n",
      "Epoch 2/10, Loss: 1.2875\n",
      "Epoch 3/10, Loss: 1.2772\n",
      "Epoch 4/10, Loss: 1.2613\n",
      "Epoch 5/10, Loss: 1.2494\n",
      "Epoch 6/10, Loss: 1.0038\n",
      "Epoch 7/10, Loss: 0.6134\n",
      "Epoch 8/10, Loss: 0.2780\n",
      "Epoch 9/10, Loss: 0.1126\n",
      "Epoch 10/10, Loss: 0.0668\n",
      "\n",
      "Final Evaluation Metrics:\n",
      "Training Accuracy: 0.9895\n",
      "Validation Accuracy: 0.9067\n",
      "Test Accuracy: 0.9474\n",
      "Training Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      1.00       161\n",
      "           1       0.98      1.00      0.99       350\n",
      "           2       1.00      0.98      0.99       250\n",
      "           3       1.00      0.97      0.98        94\n",
      "\n",
      "    accuracy                           0.99       855\n",
      "   macro avg       0.99      0.99      0.99       855\n",
      "weighted avg       0.99      0.99      0.99       855\n",
      "\n",
      "Validation Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.85      0.89        20\n",
      "           1       0.88      0.97      0.93        38\n",
      "           2       0.91      0.77      0.83        13\n",
      "           3       1.00      1.00      1.00         4\n",
      "\n",
      "    accuracy                           0.91        75\n",
      "   macro avg       0.93      0.90      0.91        75\n",
      "weighted avg       0.91      0.91      0.91        75\n",
      "\n",
      "Test Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      1.00      0.94        15\n",
      "           1       0.97      0.97      0.97        38\n",
      "           2       0.94      0.84      0.89        19\n",
      "           3       1.00      1.00      1.00         4\n",
      "\n",
      "    accuracy                           0.95        76\n",
      "   macro avg       0.95      0.95      0.95        76\n",
      "weighted avg       0.95      0.95      0.95        76\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Model initialization\n",
    "INPUT_DIM = len(vocab)  # Number of unique tokens in the vocabulary\n",
    "OUTPUT_DIM = 4  # Number of sentiment classes (0: Negative, 1: Positive, 2: Neutral, 3: Irrelevant)\n",
    "EMB_DIM = 100  # Dimensionality of the embedding layer\n",
    "HIDDEN_DIM = 256  # Number of hidden units in the RNN\n",
    "N_EPOCHS = 10  # Number of training epochs\n",
    "\n",
    "# Instantiate the encoder and decoder\n",
    "encoder = Encoder(INPUT_DIM, EMB_DIM, HIDDEN_DIM)  # Create an Encoder object\n",
    "decoder = Decoder(OUTPUT_DIM, EMB_DIM, HIDDEN_DIM)  # Create a Decoder object\n",
    "model = Seq2Seq(encoder, decoder)  # Create a Seq2Seq model\n",
    "\n",
    "# Define the main training function, which calculates metrics at the end of training\n",
    "def train_model(model, train_dataloader, valid_dataloader, test_dataloader, num_epochs, criterion, optimizer):\n",
    "    model.train()  # Set the model to training mode\n",
    "    \n",
    "    for epoch in range(num_epochs):  # Loop through each epoch\n",
    "        total_loss = 0  # Variable to accumulate total loss for this epoch\n",
    "        \n",
    "        for src, trg in train_dataloader:  # Iterate through batches in the training DataLoader\n",
    "            optimizer.zero_grad()  # Reset gradients\n",
    "            \n",
    "            # Forward pass through the model\n",
    "            output = model(src)  # Compute model predictions for the current batch\n",
    "            loss = criterion(output, trg)  # Compute loss for this batch\n",
    "            total_loss += loss.item()  # Accumulate batch loss\n",
    "            \n",
    "            # Backward pass and parameter update\n",
    "            loss.backward()  # Calculate gradients\n",
    "            optimizer.step()  # Update model parameters\n",
    "\n",
    "        # Print average loss for the epoch\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {total_loss / len(train_dataloader):.4f}\")\n",
    "\n",
    "    # Calculate and print metrics after the training loop ends\n",
    "    print(\"\\nFinal Evaluation Metrics:\")\n",
    "\n",
    "    # Calculate metrics on training set\n",
    "    train_predictions, train_true_labels = evaluate(model, train_dataloader)\n",
    "    train_accuracy = accuracy_score(train_true_labels, train_predictions)\n",
    "    print(f\"Training Accuracy: {train_accuracy:.4f}\")\n",
    "\n",
    "    # Calculate metrics on validation set\n",
    "    val_predictions, val_true_labels = evaluate(model, valid_dataloader)\n",
    "    val_accuracy = accuracy_score(val_true_labels, val_predictions)\n",
    "    print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "    # Calculate metrics on test set\n",
    "    test_predictions, test_true_labels = evaluate(model, test_dataloader)\n",
    "    test_accuracy = accuracy_score(test_true_labels, test_predictions)\n",
    "    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "    \n",
    "    print(\"Training Classification Report:\\n\", classification_report(train_true_labels, train_predictions))\n",
    "    print(\"Validation Classification Report:\\n\", classification_report(val_true_labels, val_predictions))\n",
    "    print(\"Test Classification Report:\\n\", classification_report(test_true_labels, test_predictions))\n",
    "\n",
    "# Define evaluation function for dataloaders\n",
    "def evaluate(model, dataloader):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    predictions, true_labels = [], []  # Lists to store predictions and true labels\n",
    "    \n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        for src, trg in dataloader:  # Iterate through batches in the DataLoader\n",
    "            output = model(src)  # Forward pass through the model\n",
    "            predictions.extend(output.argmax(dim=1).cpu().numpy())  # Get predicted classes\n",
    "            true_labels.extend(trg.cpu().numpy())  # Add true labels to list\n",
    "    \n",
    "    return predictions, true_labels  # Return lists of predictions and true labels\n",
    "\n",
    "# Training configuration\n",
    "criterion = nn.CrossEntropyLoss()  # Loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)  # Optimizer with learning rate\n",
    "\n",
    "# Run the training process\n",
    "train_model(model, train_dataloader, valid_dataloader, test_dataloader, N_EPOCHS, criterion, optimizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
