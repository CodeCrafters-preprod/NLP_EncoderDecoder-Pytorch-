{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# META DATA - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
    "\n",
    "    # Developer details: \n",
    "        # Name: Khushboo Mittal, Tanisha Priya, Prachi Tavse, Harshita Jangde\n",
    "        # Role: Architect\n",
    "    # Version:\n",
    "        # Version: V 1.0 (24 October 2024)\n",
    "            # Developers: Khushboo Mittal, Tanisha Priya, Prachi Tavse, Harshita Jangde\n",
    "            # Unit test: Pass\n",
    "            # Integration test: Pass\n",
    "     \n",
    "     # Description: This code snippet implements an Encoder-Decoder model for Natural Language Processing (NLP) \n",
    "     # tasks, focusing on sequence-to-sequence transformation. The model uses embedding layers for input \n",
    "     # encoding, followed by an RNN (GRU) architecture in the encoder to capture sequential dependencies. \n",
    "     # The decoder generates output sequences based on the encoded context. Applications include text translation, \n",
    "     # summarization, and question answering, with support for custom vocabularies and tokenization.\n",
    "\n",
    "# CODE - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
    "\n",
    "    # Dependency: \n",
    "        # Environment:     \n",
    "            # Python: 3.11.5\n",
    "            # torch: 2.0.1\n",
    "            # torchtext: 0.16.0\n",
    "            # scikit-learn: 1.3.2\n",
    "\n",
    "import pandas as pd  # Import pandas for data manipulation\n",
    "from sklearn.model_selection import train_test_split  # Import function for splitting datasets\n",
    "import torch  # Import PyTorch for tensor operations\n",
    "from torchtext.data.utils import get_tokenizer  # Import tokenizer utility from torchtext\n",
    "from torchtext.vocab import build_vocab_from_iterator  # Import function to build vocabulary from text\n",
    "from torch.utils.data import Dataset, DataLoader  # Import classes for dataset handling\n",
    "from torch.nn.utils.rnn import pad_sequence  # Import function to pad sequences for batching\n",
    "\n",
    "# Define column names for the dataset\n",
    "column_names = ['tweetID', 'entity', 'sentiment', 'tweet_content']\n",
    "\n",
    "# Load the dataset with specified column names; no header in the CSV\n",
    "df = pd.read_csv('../Data/twitter_sentiment_analysis.csv', names=column_names, header=None)\n",
    "df = df[['tweetID', 'sentiment', 'tweet_content']]  # Select only relevant columns\n",
    "\n",
    "# Basic preprocessing of sentiment labels and tweet content\n",
    "df['sentiment'] = df['sentiment'].map({'Positive': 1, 'Negative': 0, 'Neutral': 2, 'Irrelevant': 3})  # Encode sentiments as integers\n",
    "df['tweet_content'] = df['tweet_content'].astype(str)  # Ensure tweet content is of string type\n",
    "df.dropna(subset=['sentiment'], inplace=True)  # Remove rows with missing sentiment values\n",
    "\n",
    "# Split the data into training, validation, and test sets\n",
    "train_df, test_df = train_test_split(df, test_size=0.15, random_state=42)  # 15% of data for testing\n",
    "valid_df, test_df = train_test_split(test_df, test_size=0.5, random_state=42)  # Split remaining data into valid and test sets\n",
    "\n",
    "# Tokenization using a basic English tokenizer\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "# Function to yield tokens for vocabulary creation\n",
    "def yield_tokens(data_iter):\n",
    "    for tweet in data_iter:  # Iterate through each tweet\n",
    "        yield tokenizer(tweet)  # Tokenize and yield tokens\n",
    "\n",
    "# Build vocabulary from the tokenized tweets, including a special unknown token\n",
    "vocab = build_vocab_from_iterator(yield_tokens(df['tweet_content']), specials=[\"<unk>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])  # Set default index for unknown words to the unknown token\n",
    "\n",
    "# Prepare a custom dataset class for loading tweets and their sentiments\n",
    "class TweetDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.dataframe = dataframe  # Store the DataFrame\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)  # Return the size of the dataset\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tweet = self.dataframe.iloc[idx]['tweet_content']  # Get the tweet content\n",
    "        sentiment = self.dataframe.iloc[idx]['sentiment']  # Get the sentiment label\n",
    "        return torch.tensor(vocab(tokenizer(tweet))), torch.tensor(sentiment)  # Return tokenized tweet and sentiment as tensors\n",
    "\n",
    "# Function to collate data into batches for training\n",
    "def collate_fn(batch):\n",
    "    src_batch, trg_batch = zip(*batch)  # Unzip the batch into tweet and sentiment tensors\n",
    "    src_batch = pad_sequence(src_batch, padding_value=vocab[\"<pad>\"], batch_first=True)  # Pad the tweet sequences\n",
    "    trg_batch = torch.stack(trg_batch)  # Stack sentiment labels into a tensor\n",
    "    return src_batch, trg_batch  # Return padded tweets and labels\n",
    "\n",
    "# Create dataset instances for training, validation, and test sets\n",
    "train_dataset = TweetDataset(train_df)\n",
    "valid_dataset = TweetDataset(valid_df)\n",
    "test_dataset = TweetDataset(test_df)\n",
    "\n",
    "# Create DataLoader instances for each dataset to handle batching and shuffling\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn  # Import PyTorch's neural network module\n",
    "\n",
    "# Define the Encoder class\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hidden_dim):\n",
    "        super(Encoder, self).__init__()  # Initialize the parent class\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)  # Embedding layer to convert input tokens to vectors\n",
    "        self.rnn = nn.GRU(emb_dim, hidden_dim, batch_first=True)  # GRU layer with input dimension as embedding size\n",
    "\n",
    "    def forward(self, src):\n",
    "        embedded = self.embedding(src)  # Convert input token indices to embeddings\n",
    "        outputs, hidden = self.rnn(embedded)  # Pass embeddings through the RNN\n",
    "        return hidden  # Return the hidden state for further processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Decoder generates predictions (sentiment labels) based on the hidden state provided by the encoder and the current input token.\n",
    "parameters of intit:\n",
    "\n",
    "output_dim: The size of the output vocabulary (number of unique sentiment labels).\n",
    "\n",
    "emb_dim: The dimension of the embedding vectors (same as in the encoder).\n",
    "\n",
    "hidden_dim: The number of features in the hidden state (same as in the encoder)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Decoder class\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hidden_dim):\n",
    "        super(Decoder, self).__init__()  # Initialize the parent class\n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)  # Embedding layer for output tokens\n",
    "        self.rnn = nn.GRU(emb_dim, hidden_dim, batch_first=True)  # GRU layer for decoding\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)  # Fully connected layer to convert hidden states to output logits\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        input = input.unsqueeze(1)  # Reshape input for RNN: (batch_size, 1)\n",
    "        embedded = self.embedding(input)  # Convert input token indices to embeddings\n",
    "        output, hidden = self.rnn(embedded, hidden)  # Pass embeddings and hidden state through the RNN\n",
    "        prediction = self.fc(output.squeeze(1))  # Convert RNN output to predictions, shape: (batch_size, output_dim)\n",
    "        return prediction, hidden  # Return predictions and the new hidden state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Seq2Seq, self).__init__()  # Initialize the parent class\n",
    "        self.encoder = encoder  # Assign the encoder model\n",
    "        self.decoder = decoder  # Assign the decoder model\n",
    "\n",
    "    def forward(self, src):\n",
    "        hidden = self.encoder(src)  # Encode the source sequence and obtain the final hidden state\n",
    "        input = torch.zeros(src.size(0), dtype=torch.long).to(src.device)  # Create a start token (shape: batch_size)\n",
    "        output, _ = self.decoder(input, hidden)  # Decode using the start token and the hidden state from the encoder\n",
    "        return output  # Return the decoder's output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 1.3135970654311004\n",
      "Epoch 2/10, Loss: 1.274238732126024\n",
      "Epoch 3/10, Loss: 1.281315216311702\n",
      "Epoch 4/10, Loss: 1.282257424460517\n",
      "Epoch 5/10, Loss: 1.1883082433983132\n",
      "Epoch 6/10, Loss: 0.9435194885289228\n",
      "Epoch 7/10, Loss: 0.5825899265430592\n",
      "Epoch 8/10, Loss: 0.3843495034509235\n",
      "Epoch 9/10, Loss: 0.2033556972940763\n",
      "Epoch 10/10, Loss: 0.07845339499827889\n"
     ]
    }
   ],
   "source": [
    "# Model initialization\n",
    "INPUT_DIM = len(vocab)  # Number of unique tokens in the vocabulary\n",
    "OUTPUT_DIM = 4  # Number of sentiment classes (0: Negative, 1: Positive, 2: Neutral, 3: Irrelevant)\n",
    "EMB_DIM = 100  # Dimensionality of the embedding layer\n",
    "HIDDEN_DIM = 256  # Number of hidden units in the RNN\n",
    "N_EPOCHS = 10  # Number of training epochs\n",
    "\n",
    "# Instantiate the encoder and decoder\n",
    "encoder = Encoder(INPUT_DIM, EMB_DIM, HIDDEN_DIM)  # Create an Encoder object\n",
    "decoder = Decoder(OUTPUT_DIM, EMB_DIM, HIDDEN_DIM)  # Create a Decoder object\n",
    "model = Seq2Seq(encoder, decoder)  # Create a Seq2Seq model\n",
    "\n",
    "# Loss function and optimizer setup\n",
    "criterion = nn.CrossEntropyLoss()  # Use CrossEntropyLoss for multi-class classification\n",
    "optimizer = torch.optim.Adam(model.parameters())  # Adam optimizer for model parameters\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(N_EPOCHS):\n",
    "    model.train()  # Set the model to training mode\n",
    "    epoch_loss = 0  # Initialize epoch loss\n",
    "    for src, trg in train_dataloader:  # Iterate over batches in the training DataLoader\n",
    "        trg = trg.long()  # Ensure target labels are of type LongTensor for loss calculation\n",
    "        optimizer.zero_grad()  # Zero the gradients before the backward pass\n",
    "        output = model(src)  # Forward pass through the model\n",
    "        loss = criterion(output, trg)  # Calculate the loss between output and target\n",
    "        loss.backward()  # Backward pass to compute gradients\n",
    "        optimizer.step()  # Update model parameters\n",
    "        epoch_loss += loss.item()  # Accumulate loss for the epoch\n",
    "    # Print the average loss for the current epoch\n",
    "    print(f'Epoch {epoch + 1}/{N_EPOCHS}, Loss: {epoch_loss / len(train_dataloader)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.9066666666666666\n",
      "Test Accuracy: 0.9342105263157895\n",
      "\n",
      "Classification Report for Validation Set:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.85      0.92        20\n",
      "           1       1.00      0.89      0.94        38\n",
      "           2       0.72      1.00      0.84        13\n",
      "           3       0.67      1.00      0.80         4\n",
      "\n",
      "    accuracy                           0.91        75\n",
      "   macro avg       0.85      0.94      0.88        75\n",
      "weighted avg       0.93      0.91      0.91        75\n",
      "\n",
      "\n",
      "Classification Report for Test Set:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.93      0.90        15\n",
      "           1       1.00      0.92      0.96        38\n",
      "           2       0.86      0.95      0.90        19\n",
      "           3       1.00      1.00      1.00         4\n",
      "\n",
      "    accuracy                           0.93        76\n",
      "   macro avg       0.93      0.95      0.94        76\n",
      "weighted avg       0.94      0.93      0.94        76\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def evaluate(model, dataloader):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    predictions, true_labels = [], []  # Initialize lists to store predictions and true labels\n",
    "    with torch.no_grad():  # Disable gradient calculation for evaluation\n",
    "        for src, trg in dataloader:  # Iterate through batches in the DataLoader\n",
    "            output = model(src)  # Forward pass through the model\n",
    "            predictions.extend(output.argmax(dim=1).cpu().numpy())  # Get predicted classes and add to predictions list\n",
    "            true_labels.extend(trg.cpu().numpy())  # Add true labels to the true_labels list\n",
    "    return predictions, true_labels  # Return predictions and true labels\n",
    "\n",
    "# Evaluate on the validation set\n",
    "val_predictions, val_true = evaluate(model, valid_dataloader)  # Get predictions and true labels for validation data\n",
    "\n",
    "# Evaluate on the test set\n",
    "test_predictions, test_true = evaluate(model, test_dataloader)  # Get predictions and true labels for test data\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "from sklearn.metrics import accuracy_score, classification_report  # Import necessary metrics\n",
    "\n",
    "val_accuracy = accuracy_score(val_true, val_predictions)  # Calculate accuracy for validation set\n",
    "test_accuracy = accuracy_score(test_true, test_predictions)  # Calculate accuracy for test set\n",
    "\n",
    "# Print evaluation results\n",
    "print(\"Validation Accuracy:\", val_accuracy)  # Output validation accuracy\n",
    "print(\"Test Accuracy:\", test_accuracy)  # Output test accuracy\n",
    "print(\"\\nClassification Report for Validation Set:\\n\", classification_report(val_true, val_predictions))  # Detailed classification report for validation set\n",
    "print(\"\\nClassification Report for Test Set:\\n\", classification_report(test_true, test_predictions))  # Detailed classification report for test sets"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
