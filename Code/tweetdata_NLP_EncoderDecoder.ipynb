{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "\n",
    "# Define column names\n",
    "column_names = ['tweetID', 'entity', 'sentiment', 'tweet_content']\n",
    "\n",
    "# Load the dataset with specified column names\n",
    "df = pd.read_csv('../Data/twitter_sentiment_analysis.csv', names=column_names, header=None)\n",
    "df = df[['tweetID', 'sentiment', 'tweet_content']]  # Select relevant columns\n",
    "\n",
    "# Basic preprocessing\n",
    "df['sentiment'] = df['sentiment'].map({'Positive': 1, 'Negative': 0, 'Neutral': 2, 'Irrelevant': 3})  # Encode sentiments\n",
    "df['tweet_content'] = df['tweet_content'].astype(str)\n",
    "df.dropna(subset=['sentiment'], inplace=True)\n",
    "\n",
    "train_df, test_df = train_test_split(df, test_size=0.15, random_state=42)  # 15% for test\n",
    "valid_df, test_df = train_test_split(test_df, test_size=0.5, random_state=42) \n",
    "\n",
    "# Tokenization\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "# Create vocabulary\n",
    "def yield_tokens(data_iter):\n",
    "    for tweet in data_iter:\n",
    "        yield tokenizer(tweet)\n",
    "\n",
    "vocab = build_vocab_from_iterator(yield_tokens(df['tweet_content']), specials=[\"<unk>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])  # Default index for unknown words\n",
    "\n",
    "# Prepare dataset class\n",
    "class TweetDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.dataframe = dataframe\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tweet = self.dataframe.iloc[idx]['tweet_content']\n",
    "        sentiment = self.dataframe.iloc[idx]['sentiment']\n",
    "        return torch.tensor(vocab(tokenizer(tweet))), torch.tensor(sentiment)\n",
    "\n",
    "# Create DataLoader\n",
    "def collate_fn(batch):\n",
    "    src_batch, trg_batch = zip(*batch)\n",
    "    src_batch = pad_sequence(src_batch, padding_value=vocab[\"<pad>\"], batch_first=True)\n",
    "    trg_batch = torch.stack(trg_batch)  # Sentiment labels\n",
    "    return src_batch, trg_batch\n",
    "\n",
    "train_dataset = TweetDataset(train_df)\n",
    "valid_dataset = TweetDataset(valid_df)\n",
    "test_dataset = TweetDataset(test_df)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hidden_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        self.rnn = nn.GRU(emb_dim, hidden_dim, batch_first=True)  # Use batch_first=True\n",
    "\n",
    "    def forward(self, src):\n",
    "        embedded = self.embedding(src)\n",
    "        outputs, hidden = self.rnn(embedded)\n",
    "        return hidden\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hidden_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        self.rnn = nn.GRU(emb_dim, hidden_dim, batch_first=True)  # Use batch_first=True\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        input = input.unsqueeze(1)  # Shape: (batch_size, 1)\n",
    "        embedded = self.embedding(input)\n",
    "        output, hidden = self.rnn(embedded, hidden)\n",
    "        prediction = self.fc(output.squeeze(1))  # Shape: (batch_size, output_dim)\n",
    "        return prediction, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, src):\n",
    "        hidden = self.encoder(src)\n",
    "        input = torch.zeros(src.size(0), dtype=torch.long).to(src.device)  # Start token (shape: batch_size)\n",
    "        output, _ = self.decoder(input, hidden)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 1.3135970654311004\n",
      "Epoch 2/10, Loss: 1.274238732126024\n",
      "Epoch 3/10, Loss: 1.281315216311702\n",
      "Epoch 4/10, Loss: 1.282257424460517\n",
      "Epoch 5/10, Loss: 1.1883082433983132\n",
      "Epoch 6/10, Loss: 0.9435194885289228\n",
      "Epoch 7/10, Loss: 0.5825899265430592\n",
      "Epoch 8/10, Loss: 0.3843495034509235\n",
      "Epoch 9/10, Loss: 0.2033556972940763\n",
      "Epoch 10/10, Loss: 0.07845339499827889\n"
     ]
    }
   ],
   "source": [
    "# Model initialization\n",
    "INPUT_DIM = len(vocab)\n",
    "OUTPUT_DIM = 4  # Sentiment classes (0, 1, 2, 3)\n",
    "EMB_DIM = 100\n",
    "HIDDEN_DIM = 256\n",
    "N_EPOCHS = 10\n",
    "\n",
    "encoder = Encoder(INPUT_DIM, EMB_DIM, HIDDEN_DIM)\n",
    "decoder = Decoder(OUTPUT_DIM, EMB_DIM, HIDDEN_DIM)\n",
    "model = Seq2Seq(encoder, decoder)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(N_EPOCHS):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for src, trg in train_dataloader:\n",
    "        trg = trg.long()  # Ensure target is of type LongTensor\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src)\n",
    "        loss = criterion(output, trg)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    print(f'Epoch {epoch + 1}/{N_EPOCHS}, Loss: {epoch_loss / len(train_dataloader)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.9066666666666666\n",
      "Test Accuracy: 0.9342105263157895\n",
      "\n",
      "Classification Report for Validation Set:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.85      0.92        20\n",
      "           1       1.00      0.89      0.94        38\n",
      "           2       0.72      1.00      0.84        13\n",
      "           3       0.67      1.00      0.80         4\n",
      "\n",
      "    accuracy                           0.91        75\n",
      "   macro avg       0.85      0.94      0.88        75\n",
      "weighted avg       0.93      0.91      0.91        75\n",
      "\n",
      "\n",
      "Classification Report for Test Set:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.93      0.90        15\n",
      "           1       1.00      0.92      0.96        38\n",
      "           2       0.86      0.95      0.90        19\n",
      "           3       1.00      1.00      1.00         4\n",
      "\n",
      "    accuracy                           0.93        76\n",
      "   macro avg       0.93      0.95      0.94        76\n",
      "weighted avg       0.94      0.93      0.94        76\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def evaluate(model, dataloader):\n",
    "    model.eval()\n",
    "    predictions, true_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for src, trg in dataloader:\n",
    "            output = model(src)\n",
    "            predictions.extend(output.argmax(dim=1).cpu().numpy())  # Get predicted class\n",
    "            true_labels.extend(trg.cpu().numpy())\n",
    "    return predictions, true_labels\n",
    "\n",
    "# Evaluate on validation set\n",
    "val_predictions, val_true = evaluate(model, valid_dataloader)\n",
    "\n",
    "# Evaluate on test set\n",
    "test_predictions, test_true = evaluate(model, test_dataloader)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "val_accuracy = accuracy_score(val_true, val_predictions)\n",
    "test_accuracy = accuracy_score(test_true, test_predictions)\n",
    "\n",
    "print(\"Validation Accuracy:\", val_accuracy)\n",
    "print(\"Test Accuracy:\", test_accuracy)\n",
    "print(\"\\nClassification Report for Validation Set:\\n\", classification_report(val_true, val_predictions))\n",
    "print(\"\\nClassification Report for Test Set:\\n\", classification_report(test_true, test_predictions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
